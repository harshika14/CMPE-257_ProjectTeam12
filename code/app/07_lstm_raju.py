# -*- coding: utf-8 -*-
"""07_LSTM_Raju.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PBNOFpCS7cvR9OR9paV_FtW-5GVItvvJ
"""

# General Libraries
import json
import sys
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import itertools

# NLP
import nltk
import re
from nltk.corpus import stopwords
from bs4 import BeautifulSoup
from nltk.stem import PorterStemmer


# ML/DL
import tensorflow as tf
import pickle

from sklearn.preprocessing import LabelBinarizer, LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split, GridSearchCV

from tensorflow import keras
from keras import Sequential
from keras.layers import Dense, Activation, Dropout, Embedding, Conv1D, MaxPooling1D, LSTM, BatchNormalization, SpatialDropout1D, GRU, GlobalMaxPool1D, Bidirectional
from keras_preprocessing.sequence import pad_sequences
from keras.preprocessing import text, sequence
from keras import utils
from keras import regularizers
from keras.models import load_model
from keras.initializers import Constant
from keras.utils import plot_model
from keras.wrappers.scikit_learn import KerasClassifier

df=pd.read_csv("california_restaurants.csv")

df.head(1)

df=df[["text","review_stars"]]
df.review_stars.value_counts().plot(kind="bar")

df_test=df

to_remove = np.random.choice(df_test[df_test['review_stars']==5].index,size=80000,replace=False)
df_testv1=df_test.drop(to_remove)

df_testv1.review_stars.value_counts()

to_remove = np.random.choice(df_testv1[df_testv1['review_stars']==4].index,size=24000,replace=False)
df_testv1=df_testv1.drop(to_remove)

df_testv1.review_stars.value_counts().plot(kind="bar")

df_testv1.to_csv("manual_sample.csv")

"""#### manual sampling has been done to have an almost balanced dataset"""

X = df_testv1['text'].fillna('').values
y = df_testv1['review_stars']

X.shape

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# max_words = 3000
# tokenizer = text.Tokenizer(num_words=max_words, char_level=False)
# 
# tokenizer.fit_on_texts(X_train)
# X_train = tokenizer.texts_to_matrix(X_train)
# X_test = tokenizer.texts_to_matrix(X_test)
# 
# encoder = LabelEncoder()
# encoder.fit(y_train)
# y_train = encoder.transform(y_train)
# y_test = encoder.transform(y_test)
# 
# num_classes = np.max(y_train) + 1
# y_train = utils.to_categorical(y_train, num_classes)
# y_test = utils.to_categorical(y_test, num_classes)
# 
# print('X_train shape:', X_train.shape)
# print('X_test shape:', X_test.shape)
# print('y_train shape:', y_train.shape)
# print('y_test shape:', y_test.shape)

with open('tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)

from keras.losses import mean_absolute_error,binary_crossentropy, categorical_crossentropy

def my_custom_loss(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    crossentropy = categorical_crossentropy(y_true, y_pred)
    return mae + crossentropy

batch_size = 256
epochs = 10


optimizer = keras.optimizers.Adam(learning_rate=.0007, beta_1=0.9, beta_2=0.95, amsgrad=False)

baseline = Sequential()
baseline.add(Dense(512, input_shape=(max_words,), kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),
          bias_regularizer=regularizers.l2(1e-4),
          activity_regularizer=regularizers.l2(1e-5)))
baseline.add(BatchNormalization())
baseline.add(Activation('relu'))
baseline.add(Dropout(0.3))
baseline.add(Dense(5))
baseline.add(Activation('softmax'))

baseline.compile(loss=my_custom_loss,
              optimizer=optimizer,
              metrics=['accuracy', 'mean_absolute_error'])

history = baseline.fit(X_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    validation_split=0.2)

score = baseline.evaluate(X_test, y_test,
                       batch_size=batch_size, verbose=1)
print('Test accuracy:', score[1], "Test MAE:", score[2])

y_pred = baseline.predict(X_test)

cols = [1, 2, 3, 4, 5]

# Creating predictions table
baseline_ps = pd.DataFrame(data=y_pred, columns=cols)
y_pred_true = baseline_ps.idxmax(axis=1)

# Creating truth
baseline_truth = pd.DataFrame(data=y_test, columns=cols)
y_test_true = baseline_truth.idxmax(axis=1)

# Confusion matrix
cm = confusion_matrix(y_pred_true, y_test_true)
pd.DataFrame(cm, index=cols, columns=cols)

print(classification_report(y_pred_true, y_test_true))

def test_model(params):
    print(params)
    batch_size, epochs, learning_rate = params
    
    optimizer = keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.95, amsgrad=False)

    baseline = Sequential()
    baseline.add(Dense(512, input_shape=(max_words,), kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),
              bias_regularizer=regularizers.l2(1e-4),
              activity_regularizer=regularizers.l2(1e-5)))
    baseline.add(BatchNormalization())
    baseline.add(Activation('relu'))
    baseline.add(Dropout(0.3))
    baseline.add(Dense(5))
    baseline.add(Activation('softmax'))
    
    baseline.compile(loss=my_custom_loss,
              optimizer=optimizer,
              metrics=['accuracy', 'mean_absolute_error'])
    
    history = baseline.fit(X_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=0,
                    validation_split=0.2)
    
    score = baseline.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)
    print(score[1:])
    
    return score[1:]

!nvidia-smi

from numba import cuda 
device = cuda.get_current_device()
device.reset()

!nvidia-smi

# Commented out IPython magic to ensure Python compatibility.
# %%time
# X = df_testv1['text'].fillna('').values
# y = pd.get_dummies(df_testv1['review_stars']).values
# 
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
# print(X_train.shape, y_train.shape)
# print(X_test.shape, y_test.shape)
# max_words = 5000
# maxlen = 500
# 
# X_train = tokenizer.texts_to_sequences(X_train)
# X_test = tokenizer.texts_to_sequences(X_test)
# 
# # For the LSTM, we are going to pad our sequences
# X_train = pad_sequences(X_train, maxlen=maxlen)
# X_test = pad_sequences(X_test, maxlen=maxlen)

batch_size = 128
epochs = 8

lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=.001,
    decay_steps=10000,
    decay_rate=0.9)

optimizer = keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.99, amsgrad=False, clipvalue=.3)

lstm = Sequential()
lstm.add(Embedding(max_words, 128, input_length=maxlen))
lstm.add(SpatialDropout1D(0.2))
lstm.add(Conv1D(64, 5, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),
          bias_regularizer=regularizers.l2(1e-4)))
lstm.add(MaxPooling1D(pool_size=4))
lstm.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
lstm.add(BatchNormalization())
lstm.add(Dense(50))
lstm.add(Dense(5, activation='sigmoid'))

lstm.compile(loss=my_custom_loss,
              optimizer=optimizer,
              metrics=['accuracy', 'mean_absolute_error'])

history = lstm.fit(X_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,validation_split=0.2)

plt.title('MAE Loss')
plt.plot(history.history['mean_absolute_error'], label='train')
plt.plot(history.history['val_mean_absolute_error'], label='test')
plt.legend()
plt.show()

plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show()

y_pred = lstm.predict(X_test)
y_pred

cols = [1, 2, 3, 4, 5]

# Creating predictions table
baseline_ps = pd.DataFrame(data=y_pred, columns=cols)
y_pred_true = baseline_ps.idxmax(axis=1)
y_pred_true

# Creating truth
baseline_truth = pd.DataFrame(data=y_test, columns=cols)
y_test_true = baseline_truth.idxmax(axis=1)
y_test_true

# Confusion matrix
cm = confusion_matrix(y_pred_true, y_test_true)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import classification_report
print(classification_report(y_true=y_pred_true,y_pred= y_test_true))

disp = ConfusionMatrixDisplay(confusion_matrix=cm)

disp.plot()